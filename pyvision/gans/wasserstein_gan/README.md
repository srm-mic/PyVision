# Wasserstein GAN

This is a PyTorch 1.5.0 implementation of WGAN.

Check out the paper [here](https://arxiv.org/pdf/1701.07875.pdf).

**Requirements:**

1. Python 3.6+ 
2. Numpy 1.18.5
3. PyTorch 1.5+
4. Gdown 3.11.0
5. Matplotlib 3.2.1
6. CUDA - 10.1

**Dataset**

CelebA was used for the training of this model, which can be downloaded at [this httpURL](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). 

1. If you wish to use your own dataset, the structure should be "datasets/sub_dir/img.png". The dataset will download as a file named img_align_celeba.zip.

2. Once downloaded, create a directory named **celeba/** and extract the zip file into that directory.

3. The resulting directory structure should be:
    ```
       /path/to/celeba
         -> img_align_celeba
             -> 188242.jpg
             -> 173822.jpg
             -> 284702.jpg
             -> 537394.jpg
    ```

This is an important step because we will be using the ImageFolder dataset class, which requires there to be subdirectories in the datasetï¿½s root folder.

**Usage**

```python
from model import WassGAN

# To train the GAN with default parameters
WassGAN(run_type="train")

# To run inference using the GAN
WassGAN()
```

**Train**

To train on your own dataset:

1. Specify dataset path in wgan.py " dataroot = 'path' " line 44.
2. You can change other parameters such as batch_size, etc but we suggest to use the ones already provided.
3. Please change the number of workers defined in wgan.py " workers = " line 49. 
4. More parameters regarding training length, learning rate, etc can be changed in train.py, starting line 178.
5. Number of epochs can be changed by altering the n_epoch in model.py, line 74.

## Inference

##### Weights from a pretrained model on CelebA will be downloaded automatically if not specified elsewise.

In order to run inference on your own trained model:

1. Change set_ckpt_dir in model.py
2. You can change the number of images generated by changing the "len" parameter in model.py, line 57

## Training details

* Number of epochs: 135
* Learning rate: 0.00001
* Clamp size: 0.01
* Batch size: 64
* Gpu Used: Nvidia 1660ti 6GB
* Training time: 9 Hrs

### Current output

![Image](current_output_imgs/test36.png)
